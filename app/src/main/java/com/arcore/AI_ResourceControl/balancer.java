///
/**
 *
 *
 * For HBO, this code just shows AI task's response time and calculates the average throughput
 */

// @@@ check for  "Uncomment for Bayes auto Trigger" and uncomment those codes for auto trigger in
package com.arcore.AI_ResourceControl;
/*for HBO it collects data and checks if HBO trigger is needed*/
/**
 * From around 350 line is offloading part
 * At Around 246 line, I add my own offload optimize function
 * Author @Ze Li for Offload part
 *Edited: 09/12/2024
 */

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintWriter;
import java.net.Socket;
import java.text.SimpleDateFormat;
import java.time.LocalDate;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

import static java.lang.Math.abs;
import static java.lang.Math.max;
import static java.lang.Math.min;
import static java.lang.Math.pow;
import static java.lang.Math.sqrt;
import static java.lang.Thread.sleep;

import android.annotation.SuppressLint;
import android.util.Log;
import android.widget.TextView;

import com.google.common.collect.ArrayListMultimap;
import com.google.common.collect.Iterables;
import com.google.common.collect.ListMultimap;

import org.checkerframework.checker.index.qual.LengthOf;


public class balancer implements Runnable {

    boolean hbo_trigger=true;// this enables autonomous HBO activation , make sure it's true, I temporary made it false
    //boolean offload_trigger = true; // After first HBO, we need to trigger offloading and once one offloading done
    // we need set this flag back to false until next HBO finished
    private double sendResponseTimeCode = -1000000; //set this flag to judge the data we send to data collector server

    int binCap=7;
    private final MainActivity mInstance;
    float ref_ratio=0.5f;
    int objC;
    double sensitivity[] ;
    float objquality[];
    double tris_share[];
    double current_tris;
    Map <Integer, Double> candidate_obj;
    float []coarse_Ratios=new float[]{1f,0.8f, 0.6f , 0.4f, 0.2f, 0.05f};
    //ArrayList <ArrayList<Float>> F_profit= new ArrayList<>();
    double [][]fProfit;
    double [][] tRemainder;
    int [][] track_obj;
    int sleepTime=30;
    //float candidate_obj[] = new float[total_obj];
    double tMin[] ;
    int missCounter=3;//means at least 4 noises
    // int aiIndx;
    TextView posText_re,posText_thr,posText_q,posText_app_hbo;
    double reward=0;
    double reward_server = 0;
    double reward_locally = 0;


    // Used for collect B_t & triangle count
    int port = 3434;

    boolean offload_is_triggered = false;

    public balancer(MainActivity mInstance) {

        this.mInstance = mInstance;

        objC=mInstance.objectCount+1;
        sensitivity = new double[objC];
        tris_share = new double[objC];
        objquality= new float[objC];// 1- degradation-error
        // aiIndx=ai_index;
        //model_index;

        //ArrayList <ArrayList<Float>> F_profit= new ArrayList<>();
        fProfit= new double[objC][coarse_Ratios.length];
        tRemainder= new double[objC][coarse_Ratios.length];
        track_obj= new int[objC][coarse_Ratios.length];
        //float candidate_obj[] = new float[total_obj];
        tMin = new double[objC];


        posText_re= mInstance.findViewById(R.id.app_re);
        posText_q= mInstance.findViewById(R.id.app_quality);
        posText_thr= mInstance.findViewById(R.id.app_thr);
        // posText_mir= mInstance.findViewById(R.id.app_bt);
        posText_app_hbo  = mInstance. findViewById(R.id.app_bt);
    }
    public void setHbo_trigger(boolean fact){
        this.hbo_trigger = fact;
    }

    @SuppressLint("SuspiciousIndentation")
    @Override


    public void run() {
        Log.d("OFFLOAD_MSG", "*************Balancer********");

        boolean accmodel = true;// all AI throughput trained models and RE are accurate
        boolean accRe = true;// this is to check if the trained model for re is accurate
        boolean trainedTris = false;
        boolean trainedRT = false;
        boolean trainedRE = false;
        double maxTrisThreshold = mInstance.orgTrisAllobj;
        double minTrisThreshold = maxTrisThreshold * mInstance.coarse_Ratios[mInstance.coarse_Ratios.length - 1];
        double meanRt = 0;
        double meanThr = 0;
        //totTris;
        double meanDk = 0; // mean current dis
        double meanDkk = 0; // mean of d in the next period-> you need to cal the average of predicted d for all the objects
        double pred_meanD = mInstance.pred_meanD_cur; // for next 1 sc
        boolean fault_thr = false;
        int tris_window = 4;

        int re_window = 12;

        Log.d("OFFLOAD_MSG", "Initial Set complete");
       double period_init_tris = mInstance.total_tris;// this is the starting triangle count


        for (int i = 0; i < mInstance.objectCount; i++) {
            double cur_dis = (double) mInstance.renderArray.get(i).return_distance();
            meanDk += cur_dis;
            meanDkk += mInstance.predicted_distances.get(i).get(1); // 0: next 1 sec, 1: next 2s :  // gets the first time, next 2s-- 3s of every object, ie. d1 of every obj

        }

        int objc = mInstance.objectCount;
        //totTris = mInstance.total_tris;// this might change in the middle of call, so it's better to directly use total_tris , not this local variable


        if (objc > 0) {
            meanDk /= objc;
            meanDkk /= objc;

            //  meanDk = (double) Math.round((double) 1000 * meanDk) / 1000;
            // meanDkk = (double) Math.round((double) 1000 * meanDkk) / 1000;

            if (objc == 1) {
                mInstance.initial_meanD = meanDk;
                mInstance.initial_totT = mInstance.total_tris;
            }
            if (meanDkk == 0)
                meanDkk = meanDk;
            if (pred_meanD == 0)
                pred_meanD = meanDk; // for the first objects

        }
        Log.d("OFFLOAD_MSG", "meanDk Set Complete");


        //I got an error for regression since decimation occurs in UI thread and Modeling runs at the same time
        // solution is to start data collection after one period passes from algorithm
        // else{ // just collect data when algorithm was applied in the last period


// Train the H model for each AI task from line 127 to


        //   int variousTris = mInstance.trisMeanDisk.keySet().size();


        double avg_msred_H = 0; // is equal to sum(msr_Hi)/N -> reset every period
        double avg_est_H = 0; // is equal to sum(H^i)/N -> reset every period

        // mInstance.trisMeanDisk.put(totTris, pred_meanD); //one-time correct: should have predicted value dist => removes from the head (older data) -> to then add to the tail
// this gets above bincap sooner than the rest of lists, so, we need to keep the size not more than 5
        Log.d("OFFLOAD_MSG", "Start compute latency ");
        int Ai_count = mInstance.mList.size();
        double avg_AIlatencyPeriod=0;               // This is used for the total cost
        double avg_AIlatencyPeriod_server = 0;      // This is used for the cost of the server
        double avg_AIlatencyPeriod_local = 0;       // This is used for the tasks locally
        double[] AI_latency = new double[Ai_count];

        for (int aiIndx = 0; aiIndx < Ai_count; aiIndx++) {

            double[] t_h = mInstance.getResponseT(aiIndx);
            meanRt = t_h[0];
            meanThr = t_h[1];

            AiItemsViewModel taskView=mInstance.mList.get(aiIndx);
            int deviceIdx = taskView.getCurrentDevice();

            while (meanThr > 500 ||meanThr < 0.5) // we wanna get a correct value
            {   t_h=mInstance.getResponseT(aiIndx);
                meanThr = t_h[1];
                meanRt = t_h[0];
               // Log.d("OFFLOAD_ERR", "Thr:  " + meanThr + "RT:   " + meanRt);
                if(deviceIdx == 3) {
                    break; // Sometimes it will cause balancer crushed since GPU has its own queue
                }
            }

            if(deviceIdx == 3){
                Log.d("OFFLOAD_ERR", "Thr:  " + meanThr + "RT:   " + meanRt);
                //Since I only have one GPU, the GPU also has a queue to execute the inference
                // When we have one more inferences at same time, we need to get the true latency for the
                // waiting Task
                if(meanRt==0){
                    meanRt = mInstance.offload_task_latency[aiIndx];
                }
                else{
                    //Update new latency
                    mInstance.offload_task_latency[aiIndx] = meanRt;
                }
                
            }



            // first find the best offline AI response Time = EXPECTED RESPONSE TIme
            int indq = mInstance.excel_BestofflineAIname.indexOf(taskView.getModels().get(taskView.getCurrentModel()));
            double expected_time = mInstance.excel_BestofflineAIRT.get(indq);
            // find the actual response Time
            double actual_rpT=meanRt;
            //Log.d("OFFLOAD_ERR", " MEAN RT:  " + meanRt);

            avg_AIlatencyPeriod+=(actual_rpT-expected_time)/actual_rpT;

            if(deviceIdx == 3){
                avg_AIlatencyPeriod_server += (actual_rpT-expected_time)/actual_rpT;
            }
            else{
                avg_AIlatencyPeriod_local += (actual_rpT-expected_time)/actual_rpT;
            }

            AI_latency[aiIndx] = meanRt;


//            if (aiIndx == 0) {
//                // TextView posText = (TextView) mInstance.findViewById(R.id.rspT);
//               // posText_re.setText("RT1: " + String.valueOf(meanRt));
//            } else if (aiIndx == 1) {
//
//                //  TextView posText2 = mInstance.findViewById(R.id.rspT1);
//                //posText_q.setText("RT2: " + String.valueOf(meanRt));
//            }
////
//            else if (aiIndx == 2) {
//                //    TextView posText3 = mInstance.findViewById(R.id.rspT2);
//               // posText_thr.setText("RT3: " + String.valueOf(meanRt));
//            }


        }
        Log.d("OFFLOAD_MSG", "Latency Computation successful!");

        Log.d("OFFLOAD_MSG", "Compute Reward");

        //  Uncomment for Bayes auto Trigger
        if(mInstance.curBysIters==-1)
            mInstance.afterHbo_counter++;// count consecutive HBO activation

        mInstance.avgq = calculateMeanQuality();
        double avgAIltcy= avg_AIlatencyPeriod/ mInstance.mList.size();
        double avgQ = mInstance.avgq;
        reward =avgQ - (mInstance.reward_weight*avgAIltcy);
        reward=(double) (Math.round((double) (reward * 100))) / 100;

        if(mInstance.serverList.size() > 0 ){
            double avgAIltcy_server = avg_AIlatencyPeriod_server/mInstance.serverList.size();
            reward_server = avgQ - (mInstance.reward_weight*avgAIltcy_server);
            reward_server = (double) (Math.round((double) (reward_server * 100))) / 100;
        }

        int remain_size = mInstance.mList.size() - mInstance.serverList.size();
        if(remain_size > 0){
            double avgAIltcy_local = avg_AIlatencyPeriod_local/remain_size;
            reward_locally = avgQ - (mInstance.reward_weight * avgAIltcy_local);
            reward_locally = (double) (Math.round((double) (reward_locally * 100))) / 100;
        }

        mInstance.remin_reward = reward_locally;
        mInstance.best_BT=(double) (Math.round((double) (mInstance.best_BT * 100))) / 100;


        Log.d("OFFLOAD_MSG", "All Reward Computation success!!!!"
                                        + "  Reward: " + reward
                                        + "   Server Reward: " + reward_server
                                        + "    Local  reward:  " + reward_locally
                                        +"     OldReward:   " + mInstance.old_reward);
       // posText_app_hbo.setText("B_t: "+ String.valueOf(reward));
        //posText_thr.setText("best_BT: " + String.valueOf(mInstance.best_BT));

        // Test output of B_t and Triangle count:
//        current_tris = mInstance.total_tris;
        //System.out.println("B_t:"+reward+"  " + "Triangle Count:" + current_tris);



        // I set offload_control_flag to execute offloading once since I need to collect the data to
        //Compare with original Design
        if (mInstance.hbo_is_done && mInstance.offload_execute && mInstance.offload_control_flag) {
            Log.d("OFFLOAD_MSG", "Start Offloading");
            // This means we have done the one HBO
            // At this time, we need to run offload
            // To run offload, we need to get the kTh max latency

            // And we don't want to run hbo when we offload our task
            // Set the trigger to false first
            int kTh_Task = mInstance.kthTask;
            //Log.d("OFFLOAD_MSG", "Current Kth Task   "+ kTh_Task);
            this.hbo_trigger = false;
            // Only proceed if we haven't processed all tasks
            if (kTh_Task < Ai_count) {
                // Function to handle the task offloading logic
                handleOffloadTask(kTh_Task, reward,AI_latency);
            }

            // If all tasks are done, reset the HBO trigger and flags
            if (kTh_Task == Ai_count) {
                handleEndOfOffload(reward);
            }

            // If kTh_Task exceeds task count, reset HBO for the next optimization period
            if (kTh_Task > Ai_count) {
                resetHBOForNextPeriod();
            }
        } else if (mInstance.hbo_is_done) {
            mInstance.offload_execute = true;
            if(mInstance.deleg_req == 1){
                mInstance.offload_control_flag = true;
                Log.d("OFFLOAD_MSG", "Waiting for HBO arrangement !!!");
            }
        }



        if(mInstance.hbo_is_done == true && mInstance.deleg_req == 2){
            mInstance.curBysIters = -1;
        }


        if(hbo_trigger) {
            Log.d("OFFLOAD_MSG", "Start HBO " + mInstance.curBysIters);

            if (mInstance.best_BT != 0 && mInstance.curBysIters == -1) {// if it's not in the middle of another Bayesian
//                System.out.println(mInstance.HBO_COUNTER + " time execute HBO");

                if (mInstance.afterHbo_counter <= 3)// this is to adjust the reward and remove any noises for the first three data collected after HBO activation
                    mInstance.best_BT = (reward + mInstance.best_BT) / 2;

                double perc_error = (mInstance.best_BT - reward) / mInstance.best_BT;
                Log.d("OFFLOAD_MSG"," Perc_error" + perc_error);
                if ((perc_error > 0.05 || perc_error < -0.1) && offload_is_triggered==false)// below is the function of server button
                // if BT gets worst by object addition, error becomes higher negative, if we farther awa, error becomes positive
                {
                    mInstance.hbo_trigger_false_counter++;
                    if (mInstance.hbo_trigger_false_counter >= 3)// we won't iimmidiately trigger HBO, we'll wait till
                    {
                        Log.d("OFFLOAD_MSG", "Distance Has Changed, we will trigger HBO again");
                        mInstance.hbo_trigger_false_counter = 0;
                        // In this case, we need to reset the Task arrangement
                        for(int i = 0; i < Ai_count ;i++){
                            AiItemsViewModel tempView = mInstance.mList.get(i);
                            int device = tempView.getCurrentDevice();
                            int model_index = tempView.getCurrentModel();
                            int default_device = 1; // GPU
                            int pos = mInstance.mList.indexOf(tempView);

                            if(device == 3){
                                // we need to stop this task and set this task locally
                                // The default device is GPU
                                tempView.setCurrentDevice(default_device);
                                mInstance.runOnUiThread(()->
                                        mInstance.adapter.updateActiveModel(
                                                model_index,
                                                default_device,
                                                1,
                                                tempView,
                                                pos
                                        ));
                                mInstance.serverList.remove(tempView);

                            }
                        }

                        Log.d("HBO_MSG", "New delegate request has send");
                        ModelRequestManager.getInstance().add(new ModelRequest(mInstance.getApplicationContext(), mInstance, mInstance.deleg_req, "delegate"), false, false);
                        mInstance.deleg_req += 1;
                    }

                }
                else if(offload_is_triggered){
                    offload_is_triggered =false;
                    Log.d("OFFLOAD_MSG", "Final Bt: " + reward);
//                    Log.d("HBO_MSG", "Delegate req: " + mInstance.deleg_req);
                    Log.d("HBO_MSG", "After Offloading, we send hbo request");
                    ModelRequestManager.getInstance().add(new ModelRequest(mInstance.getApplicationContext(), mInstance, mInstance.deleg_req, "delegate"), false, false);
                    mInstance.deleg_req += 1;

                }
                else
                    mInstance.hbo_trigger_false_counter = 0;// we want to get 5 Bts consecutively with error

                // set back to trigger next time hbo

            }
        }


        Log.d("OFFLOAD_MSG", "Start to write Quality");
        writequality();
        writeRewards();
        Log.d("OFFLOAD_MSG", "write Quality and rewards success");


    }


    public int[] record_latency(double[] Ai_task_latency) {

        Integer[] indices = IntStream.range(0, Ai_task_latency.length).boxed().toArray(Integer[]::new);

        Arrays.sort(indices, Comparator.comparingDouble(i -> -Ai_task_latency[i]));

        return Arrays.stream(indices).mapToInt(Integer::intValue).toArray();
    }




    public void change_device(int Ai_indx, int previous_device){
        AiItemsViewModel curr_AI = mInstance.mList.get(Ai_indx);
        int device_index = curr_AI.getCurrentDevice();
        int model_index = curr_AI.getCurrentModel();
        int pos = mInstance.mList.indexOf(curr_AI);
        mInstance.resetTimer(Ai_indx);

        //mInstance.resetTimer(Ai_indx);
        //Log.d("OFFLOAD_MSG","Before changing Device Name: " + curr_AI.getDevices().get(curr_AI.getCurrentDevice()));
        curr_AI.setCurrentDevice(previous_device);
        //Log.d("OFFLOAD_MSG","The latency of " + curr_AI.getModels().get(curr_AI.getCurrentModel()) +" after offloading is larger");
        //Log.d("OFFLOAD_MSG","Parameter: " + curr_AI.getDevices().get(prev_device));
        //Log.d("OFFLOAD_MSG","After changing Device Name: " + curr_AI.getDevices().get(curr_AI.getCurrentDevice()));
        Log.d("OFFLOAD_MSG" , "Changed Model Name is "  + curr_AI.getModels().get(curr_AI.getCurrentModel()));
        int newDevice = curr_AI.getCurrentDevice();

        mInstance.runOnUiThread(()->
                mInstance.adapter.updateActiveModel(
                        model_index,
                        newDevice,
                        1,
                        curr_AI,
                        pos
                ));


        if(newDevice!=3){
            mInstance.serverList.remove(curr_AI);
        }
    }

    /**
     * Send Data(Bitmap) to edge server
     * @param AI_index
     * @return original_device_name
     */
    public int send_Task_To_Server(int AI_index){
        int original_Device = -1;
        if(AI_index != -1){
            AiItemsViewModel tempView = MainActivity.mList.get(AI_index);
            String model_name = tempView.getModels().get(tempView.getCurrentModel());
            original_Device = tempView.getCurrentDevice();
            String device_name = tempView.getDevices().get(original_Device);
//            System.out.println("Max AI latency period: " + AI_index + " AI Model Name: " + model_name + " Device Name: " + device_name );

            // Sending Bitmap to server
            if(!device_name.equals("SERVER")){
                mInstance.resetTimer(AI_index);
                Log.d("OFFLOAD_MSG","We are sending Task:  " + model_name);
                MainActivity.serverList.add(tempView);
                int modelIndex = tempView.getCurrentModel();
                int serverDeviceIndex = tempView.getDevices().indexOf("SERVER");
                int pos = mInstance.mList.indexOf(tempView);
//                Log.d("OFFLOAD_MSG" , "POS: " + pos + " ID:" + tempView.getID());


                tempView.setCurrentDevice(serverDeviceIndex);
                int newDevice = tempView.getCurrentDevice();
                mInstance.runOnUiThread( ()->
                        mInstance.adapter.updateActiveModel(
                                modelIndex,
                                newDevice,
                                0,
                                tempView,
                                pos
                        )
                );

            }

        }
        return  original_Device;
    }
    // Extracted methods to simplify the logic
    private void handleOffloadTask(int kTh_Task, double reward,double[] AI_latency) {

        if (mInstance.old_reward == 0) {
            // First offload, send max latency to server
            mInstance.offload_req++;
            mInstance.old_reward = reward;
            mInstance.offload_task_list = record_latency(AI_latency);
           // viewLatencyArray(mInstance.offload_task_list, AI_latency);

            int maxAiTask = mInstance.offload_task_list[kTh_Task];
            AiItemsViewModel taskModel = mInstance.mList.get(maxAiTask);
            int modelIndx = taskModel.getCurrentModel();
            mInstance.previous_device = send_Task_To_Server(maxAiTask);

        }

        else if (mInstance.old_reward > reward) {
            // If reward decreases after offloading, revert to previous task
            revertToPreviousTask(kTh_Task);
           // viewLatencyArray(mInstance.offload_task_list, AI_latency);
        } else {
            // Continue with offloading
            mInstance.old_reward = reward;
            executeTaskOffload(kTh_Task);
           // viewLatencyArray(mInstance.offload_task_list, AI_latency);
        }

        // Update kth task
        mInstance.kthTask++;
    }

    private void executeTaskOffload(int kTh_Task) {
        int taskIndex = mInstance.offload_task_list[kTh_Task];
        AiItemsViewModel taskModel = mInstance.mList.get(taskIndex);
        mInstance.previous_device = send_Task_To_Server(taskIndex);
    }

    private void revertToPreviousTask(int kTh_Task) {
        int previousTaskIndex = mInstance.offload_task_list[kTh_Task - 1];
        AiItemsViewModel previousTask = mInstance.mList.get(previousTaskIndex);

        if (previousTask.getCurrentDevice() == 3) {
//            if(previousTask.getCurrentDevice() != mInstance.previous_device) {
                change_device(previousTaskIndex, mInstance.previous_device);
                mInstance.is_changed = true;
            //}
        }
        // Move back kth task
        mInstance.kthTask--;
    }

    private void handleEndOfOffload(double reward) {
        if (mInstance.old_reward > reward) {
            int previousTaskIndex = mInstance.offload_task_list[mInstance.kthTask - 1];
            Log.d("OFFLOAD_MSG", "Start to change Device: " + mInstance.old_reward + " Current Reward: " + reward);
            change_device(previousTaskIndex, mInstance.previous_device);
        }
        mInstance.kthTask++;
    }

    private void resetHBOForNextPeriod() {
        this.hbo_trigger = true;
        offload_is_triggered = true;
        mInstance.hbo_is_done = false;
        mInstance.offload_execute = false;
        mInstance.offload_control_flag = false;  // Only del_req = 1 since we need to run once to collect data
        mInstance.curBysIters = -1;
        mInstance.kthTask = 0;
        mInstance.old_reward = 0;
    }

    // This function is used for looking the latency array
    // Just for debuging
    public void viewLatencyArray(int[] Ai, double[] latency){
        for(int i = 0; i  < Ai.length ; i++){
            AiItemsViewModel temp = mInstance.mList.get(Ai[i]);
            Log.d("OFFLOAD_MSG", "*****************Latency Array*****************");
            Log.d("OFFLOAD_MSG", "Model Name: " + temp.getModels().get(temp.getCurrentModel())
                                        + " Latency: " + latency[Ai[i]]);
        }
        Log.d("OFFLOAD_MSG", "****************************************************");
    }




    @SuppressLint("SuspiciousIndentation")
    public void run_old() {

        boolean accmodel = true;// all AI throughput trained models and RE are accurate
        boolean accRe=true;// this is to check if the trained model for re is accurate
        boolean trainedTris = false;
        boolean trainedRT = false;
        boolean trainedRE = false;
        double maxTrisThreshold = mInstance.orgTrisAllobj;
        double minTrisThreshold = maxTrisThreshold * mInstance.coarse_Ratios[mInstance.coarse_Ratios.length - 1];
        double meanRt=0;
        double meanThr=0;
        //totTris;
        double meanDk = 0; // mean current dis
        double meanDkk = 0; // mean of d in the next period-> you need to cal the average of predicted d for all the objects
        double pred_meanD = mInstance.pred_meanD_cur; // for next 1 sc
        boolean fault_thr=false;
        int tris_window=4;

        int re_window=12;


        double period_init_tris=mInstance.total_tris;// this is the starting triangle count





        for (int i = 0; i < mInstance.objectCount; i++) {
            double cur_dis=(double) mInstance.renderArray.get(i).return_distance();
            meanDk += cur_dis;
            meanDkk += mInstance.predicted_distances.get(i).get(1); // 0: next 1 sec, 1: next 2s :  // gets the first time, next 2s-- 3s of every object, ie. d1 of every obj

        }

        int objc= mInstance.objectCount;
        ///totTris = mInstance.total_tris;// this might change in the middle of call, so it's better to directly use total_tris , not this local variable


        if(objc>0) {
            meanDk /= objc;
            meanDkk /= objc;

            //  meanDk = (double) Math.round((double) 1000 * meanDk) / 1000;
            // meanDkk = (double) Math.round((double) 1000 * meanDkk) / 1000;

            if(objc==1) {
                mInstance.initial_meanD = meanDk;
                mInstance.initial_totT=mInstance.total_tris;
            }
            if (meanDkk == 0)
                meanDkk = meanDk;
            if (pred_meanD == 0)
                pred_meanD = meanDk; // for the first objects

        }



        //I got an error for regression since decimation occurs in UI thread and Modeling runs at the same time
        // solution is to start data collection after one period passes from algorithm
        // else{ // just collect data when algorithm was applied in the last period



        double sum_currentWt= 0; // this is to calculate est_weights for AI models
        double sum_baseWt= 0;
        double sum_rohT_wi=0;
        double sum_rohD_wi=0;
        double sum_delta_wi=0;

// Train the H model for each AI task from line 127 to


        //   int variousTris = mInstance.trisMeanDisk.keySet().size();


        double avg_msred_H=0; // is equal to sum(msr_Hi)/N -> reset every period
        double avg_est_H=0; // is equal to sum(H^i)/N -> reset every period

        // mInstance.trisMeanDisk.put(totTris, pred_meanD); //one-time correct: should have predicted value dist => removes from the head (older data) -> to then add to the tail
// this gets above bincap sooner than the rest of lists, so, we need to keep the size not more than 5
        double aiMaxError=0.1;
        int Ai_count= mInstance.mList.size();
        for (int aiIndx=0; aiIndx<Ai_count;aiIndx++) {

            double predRt = 0;

            double[] t_h = mInstance.getResponseT(aiIndx);
            // meanRt = mInstance.getResponseT(aiIndx);// after the objects are decimated
            meanRt = t_h[0];
            meanThr = t_h[1];

            while (meanThr > 500 ||meanThr < 0.5) // we wanna get a correct value
            { t_h=mInstance.getResponseT(aiIndx);
                meanThr = t_h[1];
                meanRt = t_h[0];
            }


            double curAvg= mInstance.avg_reponseT.get(aiIndx);

            if(curAvg==0d)// this is the inital value
                mInstance.avg_reponseT.set(aiIndx, meanRt  );
            else
                mInstance.avg_reponseT.set(aiIndx,
                        //( meanRt+  curAvg)/2);
                        (double) Math.round((double) ( ((meanRt+  curAvg)/2) * 100)) / 100);


// don't use tris-dis here since->we need to remove a record of old tris for each AI model
            int variousTris = mInstance.thr_models.get(aiIndx).keySet().size();// since  from thr_models

            if (variousTris == 1) {// this is to update the baseline throughput per models
                // we have no triangle count on screen

                if (mInstance.baseline_AIRt.get(aiIndx) == 0)
                    mInstance.baseline_AIRt.set(aiIndx, meanRt);
                else {
                    double curr_baseline = mInstance.baseline_AIRt.get(aiIndx);
                    mInstance.baseline_AIRt.set(aiIndx, (meanRt + curr_baseline) / 2);
                }
            }




// this is thr calculated using the modeling
            predRt = (mInstance.rohTLRt.get(aiIndx) * mInstance.total_tris) + (mInstance.rohDLRt.get(aiIndx) * pred_meanD) + mInstance.deltaLRt.get(aiIndx);// use predicted distance for almost current period (predicted distance for next 1 sec is the closest one we have)
            // predRt = (double) Math.round((double) predRt * 1000) / 1000;

//            writeThr(meanRt, predRt, trainedRT);// for the urrent period

            int ind = -1;
//                if (variousTris < 2) { // no object on the screen
//
//                    double fixedT=mInstance.total_tris;// the location is so important since we d
//                    if (  mInstance.thr_models.get(aiIndx).containsKey(fixedT) &&   mInstance.thr_models.get(aiIndx).get(fixedT).size() == binCap)  //This is to
//                        cleanOutArraysThr(fixedT, pred_meanD, mInstance);// cleans out the closest data to the curr one
//
//                    mInstance.thr_models.get(aiIndx).put(mInstance.total_tris, meanRt); //adds to the end of list correct:  should have real throughput for the regression
//                    // uses predicted cur distance for thr modeling
//                    mInstance.thParamList.get(aiIndx).put(mInstance.total_tris, Arrays.asList(mInstance.total_tris, pred_meanD, 1.0));
//
//
//                  //correct:  should have predicted value removes from the head (older data) -> to then adds to the tail
//                  if(aiIndx==0) // this prevents a crash since we don't add to tris-meankK everytime, just for one time + also needed to be removed in clear function one time
//                     mInstance.trisMeanDisk.put(mInstance.total_tris, pred_meanD); //should be called one-time per decision period not per AI task since it is the distance of all AI tasks
//
//                }
            // starting throughput model
//                if (variousTris >= 2) {// at least two points to start modeling

// checks error of the model after new added model
            // NEW @@@ Nil test for new window-based data collection/training -> to remove old data for training
            if(variousTris==tris_window)// I am tying to remove the current value of hashmap
            {
                // cleans out the closest data to the curr one within bins @@@ NOTE THAT ONLY mInstance.thr_models has the data ordered based on insersion
                double oldest_tris= Iterables.getFirst( mInstance.thr_models.get(aiIndx).keys(), null);//  the oldest inserted entry (the trisCount) ->
                // Map.Entry<Double, Double> entry=mInstance.thr_models.get(aiIndx).entries().iterator().next();
                mInstance.thr_models.get(aiIndx).removeAll(oldest_tris);
                mInstance.thParamList.get(aiIndx).removeAll(oldest_tris);
                mInstance.trisMeanDisk.removeAll(oldest_tris);
//                        mInstance.trisMeanDisk.removeAll(entry.getKey());

            }// removes the set of <key.list> from the head of multimap

            double fixedT=mInstance.total_tris;
            // should have data collection any way
            if ( mInstance.thr_models.get(aiIndx).containsKey(fixedT) &&
                    mInstance.thr_models.get(aiIndx).get(fixedT).size() == binCap) { //we delete from array of initial tris not new, since we have data of initial tris by 100% we keep data up to binCap per triangleCount
                cleanOutArraysThr(fixedT, pred_meanD, mInstance);// cleans out the closest data to the curr one within bins
            }




            mInstance.thr_models.get(aiIndx).put(mInstance.total_tris, meanRt); // we put the latest tris data-> should have real throughput for regression and thr param should have predicted distance
            mInstance.thParamList.get(aiIndx).put(mInstance.total_tris, Arrays.asList(mInstance.total_tris, pred_meanD, 1.0));

            if(aiIndx==0) // this prevents a crash since we don't add to tris-meankK everytime, just for one time + also needed to be removed in clear function one time
                mInstance.trisMeanDisk.put(mInstance.total_tris, pred_meanD); //should be called one-time per decision period not per AI task since it is the distance of all AI tasks


            double perAI_mape = 0.0; // mean of absolute error

            double meanH=1000/meanRt;
            double pred_H = (mInstance.rohTL.get(aiIndx) * mInstance.total_tris) + (mInstance.rohDL.get(aiIndx) * pred_meanD) + mInstance.deltaL.get(aiIndx); // fit is predicted current throughput should be predicted distance (we have next 1 sec but ok since we don't move)  for current period
            double AI_error=abs((meanH - pred_H)) / meanH;


            //double fit = mInstance.thSlope * mInstance.total_tris + mInstance.thIntercept;;
            perAI_mape = abs((meanRt - predRt)) / meanRt;// this is correct to calculate error coming from real throughput vs model

            int conseq_error_counter= mInstance.conseq_error.get(aiIndx);// num of consequent noises/errors per AI model

            if(perAI_mape>aiMaxError) {// assume we have the list of H during time for AI1: [13,14,15,13]
                //mInstance.hAI_acc.set(aiIndx, false);
                mInstance.conseq_error.set(aiIndx,conseq_error_counter+1);// this is to avoid multiple un-necessary
            }
            else if(conseq_error_counter>0)
                mInstance.conseq_error.set(aiIndx,conseq_error_counter-1);// reset since the model is fine


            // variousTris = mInstance.trisMeanDisk.keySet().size();// update various tris after data collection
            // up to here, all data is collected, now we need to check tot_tris and see if that's different from initial tris,
            // we'll skip the training


            ///    to train H model removed perAI_mape > aiMaxError-> replaced by conseq_error
            if ( ( mInstance.conseq_error.get(aiIndx) >2 || AI_error>0.1 ) && variousTris >= 3 && period_init_tris==mInstance.total_tris) {// we need points with at least two diff tris in order to generate the line

                ListMultimap<Double, List<Double>> copyRtParamList = ArrayListMultimap.create(mInstance.thParamList.get(aiIndx));// take a copy to then fill it for training up to capacity of 10
                ListMultimap<Double, Double> copytrisMeanRt = ArrayListMultimap.create(mInstance.thr_models.get(aiIndx));// take a copy to then fill it for training up to capacity of 10

                // to fill un-filled bins with mean data and have fair data training
                for (double curT : copytrisMeanRt.keySet()) {// use trisMeanDisk instead of thr_models since the former is updated slowly (for AIindex=0)
                    //   this is to calculate the mean of values in the bins

                    double mmeanTh = 0, mmeanDK = 0;

                    //if (mInstance.thr_models.get(aiIndx).get(curT).size() < binCap) //  trisMeanDisk might be different from thr_models since data collection is based on real-time tris change
                    int index1 = copyRtParamList.get(curT).size();
                    if (index1 <= binCap)
                    {   mmeanDK=   mInstance.trisMeanDisk.get(curT).stream().mapToDouble(a -> a).average().getAsDouble();
                        //int index = mInstance.trisMeanDisk.get(curT).size();
                        for (int j = index1; j < binCap; j++)
                            copyRtParamList.put(curT, Arrays.asList(curT, mmeanDK, 1.0)); // this is to calculate the mean of values in the bins so it's correct
                    }
                    else// sometimes happens that the main thParamList has excess data
                        while(index1>binCap) {
                            copyRtParamList.get(curT).remove(index1 - 1);
                            index1--;
                        }
                    //cleanOutArraysThr(curT,pred_meanD,mInstance,aiIndx);// to be cautious about size of array

                    int index2 = copytrisMeanRt.get(curT).size();
                    if (index2 <= binCap)
                    {   mmeanTh= mInstance.thr_models.get(aiIndx).get(curT).stream().mapToDouble(a -> a).average().getAsDouble();
                        for (int j = index2; j < binCap; j++)
                            copytrisMeanRt.put(curT, mmeanTh);

                    }
                    else
                        while(index2>binCap) {
                            copytrisMeanRt.get(curT).remove(index2 - 1);
                            index2--;
                        }
                    // cleanOutArraysThr(curT,pred_meanD,mInstance,aiIndx);// to be cautious about size of array


                }


                //     @@@@ here we check if we have any record for the
                //  decimated objects, if yes, we need to check the ratio of decimated iteration over added scenarios, we define a rate of 40% to make sure that we have
                //at least the record for decimated object equal to 40% of the added scenarios

                int decCount = mInstance.decTris.size();// #iterations of decimated objects
                if (decCount != 0) // means that we have at least one record of decimated objects
                {
                    int j = 0;
                    int upCount = (int) Math.ceil(mInstance.thr_factor * mInstance.objectCount);// we had addition up to the object count
                    for (int i = decCount; i < upCount; i++) {
                        if (j > mInstance.decTris.size() - 1)
                            j = 0;
                        double currentT = mInstance.decTris.get(j);// one of the triangles from the list of decimated-exp
                        List<Double> thrList = new LinkedList<>(copytrisMeanRt.get(currentT));// since copy list has full data
                        for (double th : thrList)// elements of re list
                            copytrisMeanRt.put(currentT, th);

                        List<List<Double>> thpList = new LinkedList<>(copyRtParamList.get(currentT));// since copy list has full data
                        for (List<Double> thpr : thpList)// rt parameters
                            copyRtParamList.put(currentT, thpr);
                        j += 1;

                    }

                }
                // to copy the decimated data into throughout modeling


                double[][] thRegParameters = copyRtParamList.values().stream()
                        .map(l -> l.stream().mapToDouble(Double::doubleValue).toArray())
                        .toArray(double[][]::new);

                if( mInstance.conseq_error.get(aiIndx) >2){
                    double[] rt = copytrisMeanRt.values().stream()
                            .mapToDouble(Double::doubleValue)
                            .toArray();

                    double[] y = Arrays.copyOfRange(rt, 0, rt.length); // should be real rt



                    mLinearRegression regression = new mLinearRegression(thRegParameters, y);
                    if (!Double.isNaN(regression.beta(0))) {

                        mInstance.rohTLRt.set(aiIndx,  regression.beta(0)     );
                        mInstance.rohDLRt.set(aiIndx,  regression.beta(1)   );
                        mInstance.deltaLRt.set(aiIndx, regression.beta(2)    );
                        //mInstance.thRmse = regression.rmse;
                        trainedRT = true;


                    }}


// this is per AI throughput model
                if(AI_error>0.1) {
                    double[] thr = copytrisMeanRt.values().stream().mapToDouble(doubleValue -> 1000 / doubleValue).toArray();
                    double[] y2 = Arrays.copyOfRange(thr, 0, thr.length); // should be average measured throughput

                    mLinearRegression regression2 = new mLinearRegression(thRegParameters, y2);
                    if (!Double.isNaN(regression2.beta(0))) {
                        mInstance.rohTL.set(aiIndx,  regression2.beta(0)     );
                        mInstance.rohDL.set(aiIndx,  regression2.beta(1)   );
                        mInstance.deltaL.set(aiIndx, regression2.beta(2)    );
                        // trainedThr = true;
                    }
                }

// this is per AI throughput model

                copytrisMeanRt.clear();
                copyRtParamList.clear();

                // end of modeling rt

            }//   to train H model


//                  specific AI throughput after remodeling
            predRt = (mInstance.rohTLRt.get(aiIndx) * mInstance.total_tris) + (mInstance.rohDLRt.get(aiIndx) * pred_meanD) + mInstance.deltaLRt.get(aiIndx);// use predicted distance for almost current period (predicted distance for next 1 sec is the closest one we have)
            //predRt = (double) Math.round((double) predRt * 1000) / 1000;


            double mape = abs(meanRt - predRt) / meanRt;



            if (mape > aiMaxError) {// train the model


                //mInstance.hAI_acc.set(aiIndx, false);

                if (variousTris >= 3) {
                    int cur_count = mInstance.thr_miss_counter.get(aiIndx);
                    mInstance.thr_miss_counter.set(aiIndx, cur_count + 1);
                }


                if (mInstance.thr_miss_counter.get(aiIndx) > 3 && mInstance.decTris.size() != 0) { // this is to regulate the factor of considering decimated collected data in the bins
                    if (meanRt > predRt) {
                        mInstance.thr_factor -= 0.1;// reduce the effect of decimation to increase predicted throughput
                        if (mInstance.thr_factor < 0)
                            mInstance.thr_factor = 0;
                    } else
                        mInstance.thr_factor += 0.1;// increase the effect of decimation to decrease the calculated pred_re

                }

            } else {

                mInstance.conseq_error.set(aiIndx,0);
                //mInstance.hAI_acc.set(aiIndx, true);
                mInstance.thr_miss_counter.set(aiIndx, 0);// the model works fine so we don't need to re-adjust the throughput factor for decimation data collection
                // we st the estimated weights if trained models are accurate

            }
            if( mInstance.conseq_error.get(aiIndx) <=2)
                mInstance.hAI_acc.set(aiIndx, true);
            else
                mInstance.hAI_acc.set(aiIndx, false);

            // let's say we estimate even if models are not accurate-> we then want to compare it with the measured wi
            //mape<=aiMaxError replaced by mInstance.conseq_error.get(aiIndx) <=2
            if(variousTris>=2 && mape<=aiMaxError ) {// we won't update the est_w of the incorrect model but we update the rest Ws
                double cur_est_w = sqrt(pow(mInstance.rohTLRt.get(aiIndx), 2) + pow(mInstance.rohDLRt.get(aiIndx), 2));
                // double alpha = 0.7;
                // cur_est_w = alpha * (cur_est_w) + ((1 - alpha) * mInstance.est_weights.get(aiIndx)); // smoothing function of past and current data

                mInstance.est_weights.set(aiIndx, cur_est_w);// not normalized-> new(SQRT( (rohT^2+rohD^2)) -> this has not-normalized est_weights
            }

                   /* we don't use measured weight anymore since it's not accurate to use just a start and end point to measure w
                    double ai_sensitivity;
                    if(variousTris==2)
                         ai_sensitivity=   Math.sqrt(  Math.pow (((meanRt-mInstance.baseline_AIRt.get(aiIndx))/mInstance.total_tris),2)  +
                            Math.pow   ( ((meanRt-mInstance.baseline_AIRt.get(aiIndx))/meanDk ),2 ) ) ; // AI throughput sensitivity over change in tris and distance

                    else{

                        double iniTris=mInstance.initial_totT;// tris of first object in which throughput data is collected
                        double initial_thr = mInstance.thr_models.get(aiIndx).get(iniTris).get(0);// throughput data corresponds to initial tris for each AI model
                        ai_sensitivity=   Math.sqrt(  Math.pow (((meanRt-initial_thr )/(mInstance.total_tris- iniTris ) ),2)  +
                                Math.pow   ( ((meanRt- initial_thr )/(meanDk-mInstance.initial_meanD) ),2 ) ) ; // AI throughput sensitivity over change in tris and distance

                    }

                    if(ai_sensitivity==0){
                        // stop
                        int x=0;
                    }


                    mInstance.msr_weights.set(aiIndx,ai_sensitivity);
                    // old: mInstance.msr_weights.set(aiIndx, 1- (meanRt/ mInstance.baseline_AIRt.get(aiIndx)));//not normalized-> set the measured Wi = measured current throughput over baseline
*/



//                } //Specific AI  throughput model

            if(mInstance.hAI_acc.contains(false))// if atleast one model has inaccurate throughput model, we don't train RE model below
                accmodel=false;


            avg_est_H+=pred_H;

            avg_msred_H+=meanH;

            //   writeThr(meanRt, predRt, trainedRT,aiIndx,100*mape, meanH, pred_H, 100*perAI_mape);// It has predicted and real throughput of task AI[index]


            //else
            //  fault_thr=true; // the measured throughput could be zero or a very large num--> faulty data

        }// for all AI models we train throughput
        /*   Specific AI throughput is tested and now works
         * so far, all model's est_weights are calculated  */



        avg_msred_H/=Ai_count; // this is average of all model's measured throughput
        avg_est_H/=Ai_count;//this is average of all estimated model throughput

        //  avg_est_H =  (double) Math.round((double) avg_est_H * 1000) / 1000;
        //  avg_msred_H =  (double) Math.round((double) avg_msred_H * 1000) / 1000;

        double predWeighted_thr=0;
        double msrd_Weighted_thr=0;
        int  variousTris = mInstance.trisMeanDisk.keySet().size();// update various tris after data collection

        boolean acc_throughput=true;

        if(variousTris>=2) { // at least one object on the screen
            //  double max_w = Collections.max(mInstance.est_weights);// this is max without considering magnitude, so -5<-2
            double max_w = Collections.max(  mInstance.est_weights, new Comparator<Double>() { // returns max of absolute value of weights
                @Override
                public int compare(Double x, Double y) {
                    return abs(x) < abs(y) ? -1 : 1;
                }});

//          double max_msr_w = Collections.max(  mInstance.msr_weights, new Comparator<Double>() { // returns max of absolute value of weights
//                            @Override
//                            public int compare(Double x, Double y) {
//                                return Math.abs(x) < Math.abs(y) ? -1 : 1;
//                            }});

            for (int i = 0; i < Ai_count; i++) {

                double est_wi=    (mInstance.est_weights.get(i) / max_w) ;
                mInstance.nrmest_weights.set(i, est_wi);// this has Normalized est_weights
                //  double =nrm_estW;// equal to normalized (sqrt(rohT^2 + rohD^2))
//                double nrm_msrW=  (double) Math.round((double) (mInstance.msr_weights.get(i) / max_msr_w) * 100) / 100;
//                mInstance.msr_weights.set(i, nrm_msrW);

                //@@@@@@@@@@ correct this after weights are tested, since for PAI we need measured weights not estimated
//                double ms_wi = mInstance.msr_weights.get(i); // equal to normalized (cur_thr/baseline)
                //    double msr_Rt=  mInstance.thr_models.get(i).get(mInstance.total_tris).get( mInstance.thr_models.get(i).get(mInstance.total_tris).size()-1);// get the last/current value
                double last_tris= Iterables.getLast( mInstance.thr_models.get(i).keys(), null);// to get last tris of each model, each model might have different last tris

                double msr_Rt= Iterables.getLast( mInstance.thr_models.get(i).get(last_tris), null);// last added thr

                //    mInstance.thr_models.get(i).get(last_tris).stream().mapToDouble(a -> a).average().getAsDouble(); //average of measured throughput in bin
                //mInstance.thr_models.get(i).get(totTris).stream().mapToDouble(f -> f.doubleValue()).average().getAsDouble();// average of measured throughput in bin

                // double msr_Rt = mInstance.thr_models.get(i).get(totTris).get(0);// get the throughput of current tris for each model
                sum_currentWt += (est_wi * (1000/msr_Rt) ); //est_wi instead of ms_wi
                sum_baseWt += ((est_wi  * mInstance.des_thr_weight) * (1000/mInstance.baseline_AIRt.get(i))  );//sum(Normalized Wi * Hbase)


                sum_rohT_wi += ( est_wi * mInstance.rohTL.get(i));// .rohTL is for avg throughput model per Ai --- sigma(rohT_i * wi)
                sum_rohD_wi += (est_wi * mInstance.rohDL.get(i));//sigma(rohD_i * wi)
                sum_delta_wi += (est_wi * mInstance.deltaL.get(i));//sigma(delta_i * wi)

                writeWeights(mInstance.hAI_acc.get(i),i);// write specific  weight after normalization
            }


            // check even if AI models are not accurate, the measured weighted_H and estimated Weighted_H are close?
            double total_est_weights = mInstance.nrmest_weights.stream().mapToDouble(f -> f.doubleValue()).sum();

            msrd_Weighted_thr = sum_currentWt/total_est_weights;// this is average weighted throughput of current period
            //  msrd_Weighted_thr = (double)  msrd_Weighted_thr ; // sum of estimated weights



            mInstance.rohT= sum_rohT_wi/total_est_weights;
            mInstance.rohD=sum_rohD_wi/total_est_weights;
            mInstance.delta=sum_delta_wi/total_est_weights;

            predWeighted_thr = (mInstance.rohT * mInstance.total_tris) + (mInstance.rohD* pred_meanD) + mInstance.delta;// use predicted distance for almost current period (predicted distance for next 1 sec is the closest one we have)


            // the accuracy of average measured and estimated throughput
            double model_mape = abs(avg_msred_H - avg_est_H) / avg_msred_H;
            // model_mape=(double) Math.round((double)model_mape*100)/100 ;

            write_weightedH( avg_msred_H ,avg_est_H ,msrd_Weighted_thr,predWeighted_thr,model_mape*100, accmodel);// first two are average over all models for measured and estimated

            if(model_mape >0.1)// this is for avg throughput not perAI model throughput
                acc_throughput=false;


        }
        /*************************   calculate Normalized weighted P_AI*/
        // to normalize, we need to find the maximum weight and normalize all models over that. and then we calculate sum(Wi * Hi)
// just weights are based on response time parameters and the rest includeing measured RE and estimated RE should include throughput

        double avgq = calculateMeanQuality();
        double PRoAR =  (avgq / mInstance.des_Q);

        double PRoAI = (sum_currentWt / sum_baseWt) ;// should include throughput

        double reMsrd = PRoAR / PRoAI;
        double nextTris = mInstance.total_tris;
        double algNxtTris = mInstance.total_tris;


//        if( acc_throughput){ // we start to train RE and calculate next tris if H_i models are accurate


        //sum_baseWt*=mInstance.des_weight;// multiply by desired minimum throughput weight (0.7)
        //mInstance.des_Thr= mInstance.baseline_AIRt.get()/Ai_count;

        //@********************  calculated weighted P_AI* @/
//@ * ******************* calculate average AI throughput from weighted model -> roh'T= sigma(rohT_i * w^i)/N roh'D= sigma(rohD_i * wi)/Ndelta'= sigma(delta_i * wi)/N
        //@ ******************* calculate average AI throughput from weighted model   *********** @//

        int var_tris= mInstance.trisRe.keySet().size();// no need for RE since we have the correct data point of system balance even for past data

        //this is for data collection to use current average throughput since predicted thr for first data points is not accurate
        if ( !acc_throughput && variousTris <=3 && reMsrd!= Double.NaN) {

            predWeighted_thr= msrd_Weighted_thr;
            //avg_msred_H;
        }


        else if(acc_throughput && reMsrd!= Double.NaN) {// we collect RE data if Average throughput model is accurate
            // NEW @@@ Enable W windows for RE training since we use est_thr with the same idea
            if (var_tris == re_window)// I am tying to remove the current value of hashmap
            {
                // cleans out the closest data to the curr one within bins @@@ NOTE THAT ONLY mInstance.thr_models has the data ordered based on insersion
                double oldest_tris = Iterables.getFirst(mInstance.trisRe.keys(), null);//  the oldest inserted entry (the trisCount) ->
                mInstance.trisRe.removeAll(oldest_tris);
                mInstance.reParamList.removeAll(oldest_tris);

            }// removes the set of <key.list> from the head of multimap
        }

        if(objc>0 && reMsrd!= Double.NaN){// not for zero tirangles
            //this is for data collection to use predicted throughput which is equal to msr_average for objCount<=3 and predictedThr for the rest data points
            double fixedT = mInstance.total_tris;
            if (mInstance.trisRe.containsKey(fixedT) && mInstance.trisRe.get(fixedT).size() == binCap)
                cleanOutArraysRE(fixedT, pred_meanD, mInstance);// check to remove extra value in the RE parameters list , substitue the newer one


            if( mInstance.trisRe.keySet().contains(mInstance.total_tris) && mInstance.trisRe.get(mInstance.total_tris).contains(reMsrd) ){
                Random r = new Random();
                double randomValue = 0.00001 + (0.0001 - 0.00001) * r.nextDouble();
                // double randomValue = rangeMin + (rangeMax - rangeMin) * r.nextDouble();
                mInstance.trisRe.put(mInstance.total_tris, reMsrd+randomValue  );//because tris-re doesn't allow for duplicated key-value but reparam does, so we want to keep their size equal
                // use predicted current dis and predicted current throughput in the modeling
                mInstance.reParamList.put(mInstance.total_tris, Arrays.asList(mInstance.total_tris, pred_meanD, predWeighted_thr, 1.0)); // the two pred are coming from prev period (that were predicted for current period)
            }

            else {
                mInstance.trisRe.put(mInstance.total_tris, reMsrd);//correct:  has real re should have real throughout
                mInstance.reParamList.put(mInstance.total_tris, Arrays.asList(mInstance.total_tris, pred_meanD, predWeighted_thr, 1.0)); // the two pred are coming from prev period (that were predicted for current period)
            }}
        //  sum of square error




        // this is for data training
//            if() {//to avoid wrong inf from tris=0 -> we won't have re or distance at this situation

        //* so far we train each task's throughput model individually, but RE should be trained generally using measured RE value of
        //PAI that comes from sum(Wi * AI_thr)/0.7*(sum(Wi * base_Hi) and PAR as usual
        // @ ******************  RE modeling *************
        /// need to add a condition for running this periodically to make sure XMIR est_weights are stable but not for MIR maybe
        double re_mape = 0.0;


        double fit = (mInstance.alphaT * mInstance.total_tris) + (mInstance.alphaD * pred_meanD) + (mInstance.alphaH * predWeighted_thr) + mInstance.zeta;// uses predicted modeling  for current period
        re_mape = abs(reMsrd - fit) / reMsrd;

        var_tris= mInstance.trisRe.keySet().size();



        if (re_mape > 0.10 && var_tris >= 3) {// we ignore tris=0 them we need points with at least two diff tris in order to generate the line

            ListMultimap<Double, List<Double>> copyreParamList = ArrayListMultimap.create(mInstance.reParamList);// take a copy to then fill it for training up to capacity of 10
            ListMultimap<Double, Double> copytrisRe = ArrayListMultimap.create(mInstance.trisRe);// take a copy to then fill it for training up to capacity of 10

            // This is to calculate mean of bins for distance, throughput ,...
            for (double curT : mInstance.trisRe.keySet()) {

                if (curT != 0 && mInstance.trisRe.get(curT).size() < binCap) {

                    double meanRE = 0, mmeanDK = 0, meanPrth = 0;

                    int index1 = copytrisRe.get(curT).size();
                    if (index1 <= binCap)
                    {
                        meanRE = mInstance.trisRe.get(curT).stream().mapToDouble(a -> a).average().getAsDouble();

                        // mmeanDK=   mInstance.trisMeanDisk.get(curT).stream().mapToDouble(a -> a).average().getAsDouble();
                        for (int j = index1; j < binCap; j++)
                            // copythParamList.put(curT, Arrays.asList(curT, mmeanDK, 1.0)); // this is to calculate the mean of values in the bins so it's correct
                            copytrisRe.put(curT, meanRE);
                    }
                    else// sometimes happens that the main thParamList has excess data
                        while(index1>binCap) {
                            copytrisRe.get(curT).remove(index1 - 1);
                            index1--;
                        }


                    int index2 = copyreParamList.get(curT).size();
                    if (index2 <= binCap)
                    {
                        double[][] reParL =  mInstance.reParamList.get(curT).stream().map(l -> l.stream().mapToDouble(Double::doubleValue).toArray())
                                .toArray(double[][]::new);
                        for (int i = 0; i < index2; i++) {

                            mmeanDK += reParL[i][1];
                            meanPrth += reParL[i][2];
                        }
                        mmeanDK /= index2;
                        meanPrth /= index2;


                        for (int j = index2; j < binCap; j++)
                            // copythParamList.put(curT, Arrays.asList(curT, mmeanDK, 1.0)); // this is to calculate the mean of values in the bins so it's correct
                            copyreParamList.put(curT, Arrays.asList(curT, mmeanDK, meanPrth, 1.0));
                    }
                    else// sometimes happens that the main thParamList has excess data
                        while(index2>binCap) {
                            copyreParamList.get(curT).remove(index2 - 1);
                            index2--;
                        }



                }// if <10
            }// for all the current data

            // for REEE

            //             @@@@ here we check if we have any record for the
            //  decimated objects, if yes, we need to check the ratio of decimated iteration over added scenarios, we define a rate of 40% to make sure that we have
            //   at least the record for decimated object equal to 40% of the added scenarios
            //  add the decimated data for throughput modeling too

            int decCount = mInstance.decTris.size();// #iterations of decimated objects
            if (decCount != 0) // means that we have at least one record of decimated objects
            {
                int j = 0;
                //int upCount= (int) Math.ceil(0.5 * mInstance.objectCount);


                int upCount = (int) Math.ceil(mInstance.re_factor * mInstance.objectCount);// we had addition up to the object count

                for (int i = decCount; i <= upCount; i++) {
                    if (j > mInstance.decTris.size() - 1)
                        j = 0;
                    double currentT = mInstance.decTris.get(j);// one of the triangles from the list of decimated-exp
                    List<Double> reList = new LinkedList<>(copytrisRe.get(currentT));// since copy list has full data
                    for (double re : reList)// elements of re list
                        copytrisRe.put(currentT, re);

                    List<List<Double>> repList = new LinkedList<>(copyreParamList.get(currentT));// since copy list has full data
                    for (List<Double> repr : repList)
                        copyreParamList.put(currentT, repr);
                    j += 1;

                }
            }
            // to copy the decimated data into re modeling

            double[] RE = copytrisRe.values().stream()
                    .mapToDouble(Double::doubleValue)
                    .toArray();


            double[][] reRegParameters = copyreParamList.values().stream()
                    .map(l -> l.stream().mapToDouble(Double::doubleValue).toArray())
                    .toArray(double[][]::new);

            if (var_tris >= 3) {// since it includes non-zero tris, so 3 is three objects on screen
                mLinearRegression regression = new mLinearRegression(reRegParameters, RE);
                if (!Double.isNaN(regression.beta(0))) {
                    mInstance.alphaT =  regression.beta(0) ;
                    mInstance.alphaD = regression.beta(1);
                    mInstance.alphaH = regression.beta(2);
                    mInstance.zeta = regression.beta(3);
                    trainedRE = true;
                }


            }

            reRegParameters = null;
            RE = null;

            copytrisRe.clear();
            copyreParamList.clear();

        }
//                    }

        //   nil   commented  sep 7

        // current period
        double predRE = (mInstance.alphaT * mInstance.total_tris) + (mInstance.alphaD * pred_meanD) + (mInstance.alphaH * predWeighted_thr) + mInstance.zeta; // for almost current period
        // predRE = (double) Math.round((double) predRE * 1000) / 1000;
// nill added temp

        re_mape = abs(reMsrd - predRE) / reMsrd;// log this
        if (re_mape > 0.1) {
            accRe = false;// after training we check to see if the model is accurate to then cal next triangle
            if (variousTris >= 3) // this is to regulate throughput factor for decimated values
                mInstance.re_miss_counter += 1;

            if (mInstance.re_miss_counter > missCounter && mInstance.decTris.size() != 0) {
                if (reMsrd > predRE) {
                    mInstance.re_factor -= 0.1;// reduce the effect of decimation to increase re
                    if (mInstance.re_factor < 0)
                        mInstance.re_factor = 0;

                } else
                    mInstance.re_factor += 0.1;// increase the effect of decimation to decrease the calculated pred_re

            }


        } else
            mInstance.re_miss_counter = 0; // the model works fine so we don't need to re-adjust the throughput factor for decimation data collection




        //  if (variousTris>=3 && Math.abs(deltaRe) >= 0.2 && (PRoAR < 0.7 || PRoAI < 0.7))// test and see what is the re range


        if (variousTris >= 3 && (reMsrd >= 1.20 || (reMsrd <= 0.8 && avgq != 1)))// if re is not balances (or pAR is not close to PAI, we will change the next tris count
            // the last cond (reMsrd <0.8 && avgq!=1) says that if the AI is working better than AR and AI has not in original quality so that we can increase tot tris
            mInstance.lastConscCounter++;

        else
            mInstance.lastConscCounter = 0;


        long time1 = 0;
        long time2 = 0;


        //     /* temp deactivate -> this is for next triangle count computation and odra algorithm
/*
                    if (  mInstance.activate_b && accRe  && mInstance.lastConscCounter > 2) // if both RE and all AI throughput models are accurate ,, the second condition is to skip change in nexttris for the first loop while we just had a change in tot tris
                    {

                        time1 = System.nanoTime() / 1000000; //starting first loop
                        time2 = time1;
                        double nomin = 1 - ((mInstance.alphaD + (mInstance.alphaH * mInstance.rohD)) * meanDkk)
                                - (mInstance.zeta + (mInstance.alphaH * mInstance.delta));
                        // double nomin2= 1- (mInstance.alphaD* meanDkk) -(mInstance.alphaH *)
                        double denom = mInstance.alphaT + (mInstance.rohT * mInstance.alphaH); //α + ργ
                        double tmpnextTris = (nomin / (denom));

                        if (tmpnextTris > 0 &&
                        tmpnextTris<= mInstance.orgTrisAllobj)  {

                            // temporarily inactive to not to run algo-> just wanna check nexttris values
                            if (tmpnextTris < mInstance.orgTrisAllobj && tmpnextTris >= minTrisThreshold)
                                nextTris = tmpnextTris;

                            else if (tmpnextTris < minTrisThreshold)
                                nextTris = minTrisThreshold ;

//                            else if (tmpnextTris > mInstance.orgTrisAllobj)
//                                nextTris = mInstance.orgTrisAllobj;


                           // nextTris = Math.round(nextTris * 1000) / 1000;
                            trainedTris = true;

                            try {


                                odraAlg((float) nextTris);
                                time2 = System.nanoTime() / 1000000;
                                // long t2 = System.nanoTime() / 1000000;
                                mInstance.t_loop1 = time2 - time1 - (sleepTime * (objC - 1));
                                // mInstance.t_loop2 = t2 - t1;
                                mInstance.lastConscCounter = 0;// we let the effect of change in triangle count stand for at least 4 times by reseting this counter. if you don't reset, by any chance new re might be <08 and then the orda happens again

                                if (nextTris != mInstance.total_tris && !mInstance.decTris.contains(mInstance.total_tris)) // if next tris is lower than total tris we have decimation
                                    mInstance.decTris.add(mInstance.total_tris);// add new total triangle count in the decimated list


                            } catch (InterruptedException e) {
                                e.printStackTrace();
                            }


                            algNxtTris = mInstance.total_tris;

                        }


                    }  //@ //if
*/

//                    writeRE(reMsrd, predRE, trainedRE, mInstance.total_tris, nextTris, algNxtTris, trainedTris, PRoAR, PRoAI, (accRe), mInstance.orgTrisAllobj, avgq, mInstance.t_loop1);// writes to the file


//                }           //  RE modeling and next tris

//          }// if all AI throughput models are accurate, we train RE and calculate next tris


//         }//  if( acc_throughput) // we start to train RE and calculate next tris if H_i models are accurate

        predRE = (mInstance.alphaT * mInstance.total_tris) + (mInstance.alphaD * pred_meanD) + (mInstance.alphaH * predWeighted_thr) + mInstance.zeta; // for almost current period
        re_mape = abs(reMsrd - predRE) / reMsrd;// log this
        if (re_mape > 0.1)
            accRe = false;// after training we check to see if the model is accurate to then cal next triangle

        writeRE(reMsrd, predRE, trainedRE, mInstance.total_tris, nextTris, algNxtTris, trainedTris, PRoAR, PRoAI, (accRe), mInstance.orgTrisAllobj, avgq, mInstance.t_loop1);// writes to the file
        writequality();
        //if we have objs on the screen, we start RE model & training
//
// temp deactive RE training
//


        // }   // if Nan

        // else
        // Log.d("Mean Throughput is", "not accepted");


        mInstance.pred_meanD_cur = meanDkk; // the predicted current_mean_distance for next period is saved here,
        // so we'll have the estimated current distance in next period




    }//run


    public void cleanOutArraysRE(double totTris, double predDis, MainActivity mInstance){// clear based on various distance value

        int index=0;


        double[][] reRegParameters = mInstance.reParamList.get(totTris).stream().map(l -> l.stream().mapToDouble(Double::doubleValue).toArray())
                .toArray(double[][]::new);// has four elements where distance is the second one

        index= findClosestRE(reRegParameters , predDis);// the index of value needed to be deleted

        for (int i=binCap-1; i<mInstance.trisRe.get(totTris).size();i++)
            // since we add if objcount != zero to avoid wrong inf from tris=0 -> we won't have re or distance at this situation
            if(index<mInstance.trisRe.get(totTris).size())
            {
                double value = Iterables.get(mInstance.trisRe.get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.trisRe.get(totTris).remove(value);
                //  List<Double> value2 = Iterables.get(mInstance.reParamList.get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.reParamList.get(totTris).remove(index);}
        // return index;
    }



    public void cleanOutArraysREFIFO(double totTris, double predDis, MainActivity mInstance){// clear based on FIFO

        int index=0;
        double value = Iterables.get(mInstance.trisRe.get(totTris), index);// gets the value in thr_models (hashmultimap)
        mInstance.trisRe.get(totTris).remove(value);
        List<Double> value2 = Iterables.get(mInstance.reParamList.get(totTris), index);// gets the value in thr_models (hashmultimap)
        mInstance.reParamList.get(totTris).remove(value2);
        // return index;
    }


    public int cleanOutArraysThrFIFO(double totTris, double predDis, MainActivity mInstance){// FIFO policy to get the updated data

//        while( mInstance.trisMeanDisk.get(totTris).size()>binCap) // to make sure if it doesn't have extra data
//            mInstance.trisMeanDisk.get(totTris).remove(0);

        int index=0; // all multimaps preserve value of a key, so we know any index  would apply for all lists

        int Ai_count= mInstance.mList.size();

        for (int aiIndx=0;aiIndx<Ai_count;aiIndx++)// we need to do it for all AIs here, since if we remove from general array of trisMeanDisk for AI_index=0, the next AIs won't have the information of tris-meandistance
            for (int i=binCap-1; i<mInstance.thr_models.get(aiIndx).get(totTris).size();i++) {

                double value = Iterables.get(mInstance.thr_models.get(aiIndx).get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.thr_models.get(aiIndx).get(totTris).remove(value); // This code removes the value not the older one that had remove(index)

                List<Double> value2 = Iterables.get(mInstance.thParamList.get(aiIndx).get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.thParamList.get(aiIndx).get(totTris).remove(value2);

                //mInstance.thParamList.get(aiIndx).get(totTris).remove(index);
                if (aiIndx == 0)// since we add to this just once at each period when aiindex==0
                    mInstance.trisMeanDisk.get(totTris).remove(index); //removes from the head (older data) -> to then add to the tail
            }

        return  index;// not used
    }

    public int cleanOutArraysThr(double totTris, double predDis, MainActivity mInstance){// if tris0meandisk is full, it means
        // the array of other AI tasks is full too, so we update the lists


        while( mInstance.trisMeanDisk.get(totTris).size()>binCap) // to make sure if it doesn't have extra data
            mInstance.trisMeanDisk.get(totTris).remove(0);

        int index=-1; // all multimaps preserve value of a key, so we know any index  would apply for all lists
        double[] disArray = mInstance.trisMeanDisk.get(totTris).stream()
                .mapToDouble(Double::doubleValue)
                .toArray(); // this has the array of predicted distance
        index = findClosest(disArray, predDis);// the index of value(closest to current mean dis) needed to be deleted

        int Ai_count= mInstance.mList.size();

        for (int aiIndx=0;aiIndx<Ai_count;aiIndx++)// we need to do it for all AIs here, since if we remove from general array of trisMeanDisk for AI_index=0, the next AIs won't have the information of tris-meandistance
            for (int i=binCap-1; i<mInstance.thr_models.get(aiIndx).get(totTris).size();i++) {

                double value = Iterables.get(mInstance.thr_models.get(aiIndx).get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.thr_models.get(aiIndx).get(totTris).remove(value); // This code removes the value not the older one that had remove(index)
                //  List<Double> value2 = Iterables.get(mInstance.thParamList.get(aiIndx).get(totTris), index);// gets the value in thr_models (hashmultimap)
                mInstance.thParamList.get(aiIndx).get(totTris).remove(index);

                //mInstance.thParamList.get(aiIndx).get(totTris).remove(index);
                if (aiIndx == 0)// since we add to this just once at each period when aiindex==0
                    mInstance.trisMeanDisk.get(totTris).remove(index); //removes from the head (older data) -> to then add to the tail
            }

        return  index;// not used
    }


    public int cleanOutArraysThr_weighted(double totTris, double predDis, MainActivity mInstance){

        int index=-1;
        if ( mInstance.trisMeanDisk_Mir.get(totTris).size() >= binCap) {

            double[] disArray = mInstance.trisMeanDisk_Mir.get(totTris).stream()
                    .mapToDouble(Double::doubleValue)
                    .toArray(); // this has the array of predicted distance

            index = findClosest(disArray, predDis);// the index of value(closest to current mean dis) needed to be deleted
            mInstance.trisMeanThr.get(totTris).remove(index); // has the real throughput
            mInstance.thParamList_Mir.get(totTris).remove(index);


            mInstance.trisMeanDisk_Mir.get(totTris).remove(index); //removes from the head (older data) -> to then add to the tail
            // mInstance.trisMeanDiskk.get(totTris).remove(index);
        }

        return  index;
    }


    public static int findClosestRE(double[][] arr, double target) { // find the closest index of arr to value distance= target to then substitue that with the newer one
        int idx = 0;
        double dist = Math.abs(arr[0][1] - target);

        for (int i = 1; i< arr.length; i++) {
            double cdist = Math.abs(arr[i][1] - target);

            if (cdist < dist) {
                idx = i;
                dist = cdist;
            }
        }

        return idx;
    }


    public static int findClosest(double[] arr, double target) { // find the closest index of arr to value distance= target to then substitue that with the newer one
        int idx = 0;
        double dist = Math.abs(arr[0] - target);

        for (int i = 1; i< arr.length; i++) {
            double cdist = Math.abs(arr[i] - target);

            if (cdist < dist) {
                idx = i;
                dist = cdist;
            }
        }

        return idx;
    }

    public void writeRewards(){
        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "Rewards"+mInstance. fileseries+".csv";
        File file = new File(FILEPATH);
        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {


            if (file.length() == 0) {
                StringBuilder header = new StringBuilder();
                header.append("Timestamp,Reward,Reward_Server,Reward_Locally,Best_BT,Reward_Difference_Percentage\n");
                writer.write(header.toString());
            }
                StringBuilder sb = new StringBuilder();
                sb.append(dateFormat.format(new Date()));
                sb.append(',');
                sb.append(reward);
                sb.append(',');
                sb.append(reward_server);
                sb.append(',');
                sb.append(reward_locally);
                sb.append(',');
                sb.append(mInstance.best_BT);
                sb.append(',');
                sb.append((mInstance.best_BT-reward)/mInstance.best_BT);
                sb.append('\n');
                writer.write(sb.toString());

        }catch (FileNotFoundException e) {
            System.out.println(e.getMessage());
        }

    }

    public void writequality(){


        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "Quality"+mInstance. fileseries+".csv";

        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
            for (int i=0; i<objC-1; i++) {
                double curtris = mInstance.renderArray.get(i).orig_tris * mInstance.ratioArray.get(i);
                float r1 = mInstance.ratioArray.get(i); // current object decimation ratio
//               if (mInstance.renderArray.get(i).fileName.contains("0.6")) // third scenario has ratio 0.6
//                   r1 = 0.6f; // jsut for scenario3 objects are decimated
//               else if(mInstance.renderArray.get(i).fileName.contains("0.3")) // sixth scenario has ratio 0.3
//                   r1=0.3f;


                float r2 = ref_ratio * r1; // wanna compare obj level of sensitivity to see if we decimate object more -> to (ref *curr) ratio, would the current object hurt more than the other ones?
                int indq = mInstance.excelname.indexOf(mInstance.renderArray.get(i).fileName);// search in excel file to find the name of current object and get access to the index of current object
                // excel file has all information for the degredation model
                float gamma = mInstance.excel_gamma.get(indq);
                float a = mInstance.excel_alpha.get(indq);
                float b = mInstance.excel_betta.get(indq);
                float c = mInstance.excel_c.get(indq);
                float d_k = mInstance.renderArray.get(i).return_distance();// current distance

                float tmper1 = Calculate_deg_er(a, b, c, d_k, gamma, r1); // deg error for current sit
                float tmper2 = Calculate_deg_er(a, b, c, d_k, gamma, r2); // deg error for more decimated obj


                float max_nrmd = mInstance.excel_maxd.get(indq);
                tmper1 = tmper1 / max_nrmd; // normalized
                tmper2= tmper2 /max_nrmd;

                if (tmper2 < 0)
                    tmper2 = 0;

                //Qi−Qi,r divided by Ti(1−Rr) = (1-er1) - (1-er2) / ....
                sensitivity[i] = (abs(tmper2 - tmper1) / (curtris - (ref_ratio * curtris)));
                tmper1 = (float) Math.round((float) (tmper1 * 1000)) / 1000;

                StringBuilder sb = new StringBuilder();
                sb.append(dateFormat.format(new Date()));
                sb.append(',');
                sb.append(mInstance.renderArray.get(i).fileName+"_n"+(i+1)+"_d"+(d_k));
                sb.append(',');
                sb.append(sensitivity[i]);
                sb.append(',');
                sb.append(r1);
                sb.append(',');
                sb.append(1-tmper1);
                sb.append(',');
                sb.append(reward);
                sb.append(',');
                sb.append(mInstance.best_BT);
                sb.append(',');
                sb.append((mInstance.best_BT-reward)/mInstance.best_BT);
                int hbo_running=0;// if hbo is not running, hence we have good data to show
                if(mInstance.curBysIters!=-1)
                    hbo_running=1;
                //  sb.append(mInstance.tasks.toString());
                sb.append(',');
                sb.append(hbo_running);


                sb.append(',');
                sb.append(mInstance.afterHbo_counter);
                sb.append('\n');
                writer.write(sb.toString());
//                System.out.println("done!");
            }
        }catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
        }

    }
//    public void writeThr(double realThr, double predThr, boolean trainedFlag,int aiIndx,double ai_acc,double meanH, double pred_H,double perAI_mape){ // AI throughput information for each task individually and response time for all models
//
//        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss:SSS");
//        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
//        String FILEPATH = currentFolder + File.separator + "Throughput"+mInstance. fileseries+".csv";
//
//        StringBuilder sb = new StringBuilder();
//        sb.append(dateFormat.format(new Date())); sb.append(',').append((aiIndx)).append(",");
//        sb.append(realThr);sb.append(',').append(predThr);sb.append(',').append(trainedFlag);sb.append(',').append(ai_acc).append(",");
//        sb.append(mInstance.rohTLRt.get(aiIndx));sb.append(',').append(mInstance.rohDLRt.get(aiIndx));sb.append(',').append(mInstance.deltaLRt.get(aiIndx));sb.append(',');
//        sb.append(mInstance.baseline_AIRt.get(aiIndx)* mInstance.des_Rt_weight);
//        sb.append(','); sb.append(mInstance.des_Q).append(',');
//        sb.append(mInstance.total_tris);
//      //  sb.append(',').append( meanthr);// this is measured directly
//        sb.append(','); sb.append(meanH).append(',').append(pred_H).append(',').append(perAI_mape);
//
//        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
//
//
//        for (int i=0;i<mInstance.mList.size();i++)
//        {
//            AiItemsViewModel taskView=mInstance.mList.get(i);
//            sb.append(",").append(taskView.getModels().get(taskView.getCurrentModel()))
//                    .append(",").append(taskView.getDevices().get(taskView.getCurrentDevice()))
//                    .append(",").append(taskView.getInferenceT()).append(",").append(taskView.getOverheadT());
//
//
//        }
//
//
//
//
//
//            sb.append('\n');
//            writer.write(sb.toString());
//            System.out.println("done!");
//        } catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
//        }
//    }


    public void writeThr(double realThr, double predThr, boolean trainedFlag,double ai_acc){


        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss:SSS");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "Throughput"+mInstance. fileseries+".csv";
        StringBuilder sb = new StringBuilder();



        sb.append(dateFormat.format(new Date())); sb.append(",,");
        sb.append(realThr);sb.append(',').append(predThr);sb.append(',').append(trainedFlag);sb.append(',').append(ai_acc).append(",");
        sb.append(" , , ,");// for weights
        // sb.append(mInstance.rohTL.get(aiIndx));sb.append(',').append(mInstance.rohDL.get(aiIndx));sb.append(',').append(mInstance.deltaL.get(aiIndx));sb.append(',');
        sb.append(mInstance.des_Thr);
        sb.append(','); sb.append(mInstance.des_Q).append(',');
        sb.append(mInstance.total_tris);

        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
            for (AiItemsViewModel taskView :mInstance.mList) {

                sb.append(",").append(taskView.getModels().get(taskView.getCurrentModel()))
                        .append(",").append(taskView.getDevices().get(taskView.getCurrentDevice()))
                        .append(",").append(taskView.getInferenceT()).append(",").append(taskView.getOverheadT());
            }
            sb.append('\n');
            writer.write(sb.toString());
//            System.out.println("done!");
        } catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
        }
    }
    public void writeWeights( boolean ai_acc,int ai_indx){ // AI throughput information for each task individually and response time for all models

        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss:SSS");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "Weights"+mInstance. fileseries+".csv";
        StringBuilder sb = new StringBuilder();
        sb.append(dateFormat.format(new Date())); sb.append(',').append(ai_acc);
        AiItemsViewModel taskView=mInstance.mList.get(ai_indx);
        sb .append(",").append(taskView.getModels().get(taskView.getCurrentModel()))
                .append(" ,")  .append(",").append(mInstance.nrmest_weights.get(ai_indx));
        sb.append('\n');

        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
            writer.write(sb.toString());
//            System.out.println("done!");
        } catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
        }
    }


    public void write_weightedH( double avg_msr_H, double avg_est_H, double msr, double pre, double Mape,boolean acc_model){ // AI throughput information for each task individually and response time for all models

        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss:SSS");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "Weighted_throughput"+mInstance. fileseries+".csv";
        StringBuilder sb = new StringBuilder();
        sb.append(dateFormat.format(new Date())); sb.append(',').append(    avg_msr_H+","+avg_est_H+","+Mape+
                ","+msr+","+pre+","+acc_model);

        sb.append('\n');
        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
            writer.write(sb.toString());
//            System.out.println("done!");
        } catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
        }
    }


    // public void writeRE(double realRe, double predRe, boolean trainedFlag,double totT, double nextT, boolean trainedT, double pAR, double pAI){
    public void writeRE(double realRe, double predRe, boolean trainedFlag, double totT, double nextT, double algTris, boolean trainedT,
                        double pAR, double pAI, boolean accM, double totTris, double avgq, long duration){

        SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss:SSS");
        String currentFolder = mInstance.getExternalFilesDir(null).getAbsolutePath();
        String FILEPATH = currentFolder + File.separator + "RE"+mInstance. fileseries+".csv";

        try (PrintWriter writer = new PrintWriter(new FileOutputStream(FILEPATH, true))) {
            StringBuilder sb = new StringBuilder();
            sb.append(dateFormat.format(new Date())); sb.append(',');
            sb.append(realRe);sb.append(',');
            sb.append(predRe);
            sb.append(',');  sb.append(trainedFlag);
            sb.append(',');  sb.append(totT);
            sb.append(',');  sb.append(nextT);
            sb.append(',');  sb.append(algTris);
            sb.append(',');  sb.append(trainedT);
            sb.append(',');  sb.append(pAR);
            sb.append(',');  sb.append(pAI);
            sb.append(',');  sb.append(accM);// if both models are accurate
            sb.append(',');  sb.append(totTris);// if both models are accurate
            sb.append(',');  sb.append(avgq);
            sb.append(',');  sb.append(duration);
            sb.append('\n');
            writer.write(sb.toString());
//            System.out.println("done!");
        } catch (FileNotFoundException e) {
//            System.out.println(e.getMessage());
        }
    }



    public float calculateMeanQuality( ) {

        float sumQual=0;
//        objquality = new float[mInstance.objectCount + 1];  // sometimes will cause some wrong
        for (int ind = 0; ind < mInstance.objectCount; ind++)
        {
            int i =  mInstance.excelname.indexOf( mInstance.renderArray.get(ind).fileName);
            float gamma = mInstance.excel_gamma.get(i);
            float a = mInstance.excel_alpha.get(i);
            float b = mInstance.excel_betta.get(i);
            float c = mInstance.excel_c.get(i);
            float d = mInstance.renderArray.get(ind).return_distance();
            float curQ = mInstance.ratioArray.get(ind);


            float deg_error = (float) Math.round((float) (Calculate_deg_er(a, b, c, d, gamma, curQ) * 1000)) / 1000;
            float max_nrmd = mInstance.excel_maxd.get(i);

            float cur_degerror = deg_error / max_nrmd;
            float quality= 1- cur_degerror;
            objquality[ind]=quality;
            sumQual+=quality;


        }


        return sumQual/mInstance.objectCount;



    }

    public float Calculate_deg_er(float a,float b,float creal,float d,float gamma, float r1) {

        float error;
        if(r1==1)
            return  0f;
        error = (float) (((a * Math.pow(r1,2)) + (b * r1) + creal) / (Math.pow(d , gamma)));
        return error;
    }



    float odraAlg(float tUP) throws InterruptedException {


        candidate_obj = new HashMap<>();
        Map<Integer, Double> sortedcandidate_obj = new HashMap<>();
        float sum_org_tris = 0; // sum of all tris of the objects o the screen

        for (int ind = 0; ind < mInstance.objectCount; ind++) {

            sum_org_tris += mInstance.renderArray.get(ind).orig_tris;// this will ne used to cal min of tris needed at each row (object) in bellow


            double curtris = mInstance.renderArray.get(ind).orig_tris * mInstance.ratioArray.get(ind);
            current_tris = curtris;


            tris_share[ind] = (curtris / tUP);
            candidate_obj.put(ind, sensitivity[ind] / tris_share[ind]);


        }
        sortedcandidate_obj = sortByValue(candidate_obj, false); // second arg is for order-> ascending or not? NO
        // Up to here, the candidate objects are known


        double updated_sum_org_tris = sum_org_tris; // keeps the last value which is sum_org_tris - tris1-tris2-....
        for (int i : sortedcandidate_obj.keySet()) { // check this gets the candidate object index to calculate min weight
            double sum_org_tris_minus = updated_sum_org_tris - mInstance.renderArray.get(i).orig_tris; // this is summ of tris for all the objects except the current one
            updated_sum_org_tris = sum_org_tris_minus;
            tMin[i] = coarse_Ratios[coarse_Ratios.length - 1] * sum_org_tris_minus;// minimum tris needs for object i+1 to object n
            ///@@@@ if this line works lonely, delete the extra line for the last object to zero in the alg
        }

        Map.Entry<Integer, Double> entry = sortedcandidate_obj.entrySet().iterator().next();
        int key = entry.getKey(); // get access to the first key -> to see if it is the first object for bellow code

        int prevInd = 0;
        for (int i : sortedcandidate_obj.keySet()){  // line 10 i here is equal to alphai -> the obj with largest candidacy
            // check this gets the candidate object index to maintain its quality
            for (int j = 0; j < coarse_Ratios.length; j++) {

                int indq = mInstance.excelname.indexOf(mInstance.renderArray.get(i).fileName);// search in excel file to find the name of current object and get access to the index of current object
                float gamma =mInstance. excel_gamma.get(indq);
                float a =mInstance. excel_alpha.get(indq);
                float b = mInstance.excel_betta.get(indq);
                float c =mInstance. excel_c.get(indq);
                float d_k = mInstance.renderArray.get(i).return_distance();// current distance
                float max_nrmd = mInstance.excel_maxd.get(indq);
                float quality = 1 -( Calculate_deg_er(a, b, c, d_k, gamma, coarse_Ratios[j]) / max_nrmd  ); // deg error for current sit

                if (i == key && tUP >= mInstance.renderArray.get(i).getOrg_tris() * coarse_Ratios[j]) { // the first object in the candidate list
                    fProfit[i][j] = quality;// Fα(i),j ←Qα(i),j -> i is alpha i
                    tRemainder[i][j] = tUP - (mInstance.renderArray.get(i).getOrg_tris() * coarse_Ratios[j]);
                } else //  here is the dynamic programming section
                    for (int s = 0; s < coarse_Ratios.length; s++) {

                        double f = fProfit[prevInd][s] + quality;
                        double t = tRemainder[prevInd][s] - (mInstance.renderArray.get(i).getOrg_tris() * coarse_Ratios[j]);
                        if (t >= tMin[i] && fProfit[i][j] < f) {

                            fProfit[i][j] = f;
                            tRemainder[i][j] = t;
                            track_obj[i][j] = s;
                        }

                    }//

            }//for j  up to here we reach line 25
            prevInd=i;
        }// for i
/// start with object with least priority

        sortedcandidate_obj = sortByValue(candidate_obj, true); // to iterate through the list from lowest to highest values

        int lowPobjIndx = sortedcandidate_obj.entrySet().iterator().next().getKey(); // line 26
        double tmp=fProfit[lowPobjIndx][0];
        int j=0;
        for  (int maxindex=1;maxindex<coarse_Ratios.length;maxindex++) // line 27
            if(fProfit[lowPobjIndx][maxindex]>tmp)// finds the index of coarse-grain ratio with maximum profit
            {
                tmp = fProfit[lowPobjIndx][maxindex];
                j=maxindex;
            }


        for (int i : sortedcandidate_obj.keySet())
        //if ( mInstance.renderArray.size()>i &&  mInstance.renderArray.get(i)!=null)

        {// to avoid null pointer error

            mInstance.total_tris = mInstance.total_tris - (mInstance.ratioArray.get(i) * (mInstance.o_tris.get(i)));// total =total -1*objtris
            mInstance.ratioArray.set(i, coarse_Ratios[j]);


            mInstance.runOnUiThread(() -> mInstance.renderArray.get(i).decimatedModelRequest(mInstance.ratioArray.get(i), i, false));

            mInstance.total_tris = mInstance.total_tris + (mInstance.ratioArray.get(i) *  mInstance.renderArray.get(i).orig_tris);// total = total + 0.8*objtris
            // mInstance.trisDec.put(mInstance.total_tris,true);

            j = track_obj[i][j];
            Thread.sleep(sleepTime);// added to prevent the crash happens while redrawing all the objects at the same time


        }


        return (float) mInstance.total_tris; // this returns the total algorithm triangle count



    }


    private static Map<Integer, Double> sortByValue(Map<Integer, Double> unsortMap, final boolean order)
    {
        List<Map.Entry<Integer, Double>> list = new LinkedList<>(unsortMap.entrySet());

        // Sorting the list based on values
        list.sort((o1, o2) -> order ? o1.getValue().compareTo(o2.getValue()) == 0
                ? o1.getKey().compareTo(o2.getKey())
                : o1.getValue().compareTo(o2.getValue()) : o2.getValue().compareTo(o1.getValue()) == 0
                ? o2.getKey().compareTo(o1.getKey())
                : o2.getValue().compareTo(o1.getValue()));
        return list.stream().collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (a, b) -> b, LinkedHashMap::new));

    }




}


